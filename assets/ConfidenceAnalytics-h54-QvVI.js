import{g as C,j as e}from"./index-D1QpPieo.js";import{a as d}from"./vendor-cytoscape-hdah7_Xt.js";import{c as R,q as L,d as E,w as S,e as D,t as T,f as $}from"./vendor-firebase-DNc0QRT5.js";import"./data-courses-Y_tFOtPD.js";function i({text:a,children:h}){return e.jsxs("span",{className:"ca-tip-wrap",children:[h,e.jsx("span",{className:"ca-tip-icon",title:a,children:"‚ìò"}),e.jsx("span",{className:"ca-tip-popup",children:a})]})}function Q(){const[a,h]=d.useState([]),[u,j]=d.useState(!0),[y,v]=d.useState(null),[m,b]=d.useState(100),p=d.useCallback(async()=>{j(!0),v(null);try{const s=C(),t=R(s),x=L(E(t,"apiUsage"),S("type","==","confidence_routing"),D("timestamp","desc"),T(m)),_=(await $(x)).docs.map(f=>({id:f.id,...f.data(),timestamp:f.data().timestamp?.toDate?.()||null}));h(_)}catch(s){console.error("Failed to load confidence analytics:",s),v(s.message)}finally{j(!1)}},[m]);d.useEffect(()=>{p()},[p]);const r=a.length,n=a.filter(s=>s.outcome==="clarify").length,l=a.filter(s=>s.outcome==="direct_answer").length,o=a.filter(s=>s.outcome==="agentic_rag").length,q=r>0?(a.reduce((s,t)=>s+(t.score||0),0)/r).toFixed(1):"‚Äî",A=r>0?Math.round(a.reduce((s,t)=>s+(t.queryLength||0),0)/r):"‚Äî",c=s=>r>0?(s/r*100).toFixed(1):"0",g={};a.forEach(s=>{(s.reasons||[]).forEach(t=>{g[t]=(g[t]||0)+1})});const N=Object.entries(g).sort((s,t)=>t[1]-s[1]).slice(0,8);return e.jsxs("div",{className:"ca-container",children:[e.jsxs("div",{className:"ca-header",children:[e.jsxs("h3",{children:["üß† Confidence Routing Analytics",e.jsx("span",{className:"ca-header-subtitle",children:"How the AI decides whether to clarify or answer directly"})]}),e.jsxs("div",{className:"ca-controls",children:[e.jsxs("select",{value:m,onChange:s=>b(Number(s.target.value)),className:"ca-limit-select",children:[e.jsx("option",{value:50,children:"Last 50"}),e.jsx("option",{value:100,children:"Last 100"}),e.jsx("option",{value:250,children:"Last 250"}),e.jsx("option",{value:500,children:"Last 500"})]}),e.jsx("button",{className:"ca-refresh-btn",onClick:p,disabled:u,children:"üîÑ Refresh"})]})]}),y&&e.jsxs("div",{className:"ca-error",children:["‚ö†Ô∏è ",y]}),u?e.jsx("div",{className:"ca-loading",children:"Loading analytics‚Ä¶"}):r===0?e.jsx("div",{className:"ca-empty",children:'No confidence routing data yet. Submit a query on the "Fix a Problem" tab to start collecting analytics.'}):e.jsxs(e.Fragment,{children:[e.jsxs("div",{className:"ca-cards ca-cards-top",children:[e.jsxs("div",{className:"ca-card",children:[e.jsx(i,{text:"Total number of diagnostic queries processed by the AI system in this time period.",children:e.jsx("span",{className:"ca-card-value",children:r})}),e.jsx("span",{className:"ca-card-label",children:"Total Queries"})]}),e.jsxs("div",{className:"ca-card ca-card-clarify",children:[e.jsx(i,{text:"Percentage of queries where the AI asked a follow-up clarification question before answering. Higher means users are asking vague questions.",children:e.jsxs("span",{className:"ca-card-value",children:[c(n),"%"]})}),e.jsxs("span",{className:"ca-card-label",children:["Clarified (",n,")"]})]}),e.jsxs("div",{className:"ca-card ca-card-direct",children:[e.jsx(i,{text:"Percentage of queries where the AI had enough context to answer directly without asking follow-up questions. Higher is better ‚Äî means users are providing clear, detailed queries.",children:e.jsxs("span",{className:"ca-card-value",children:[c(l),"%"]})}),e.jsxs("span",{className:"ca-card-label",children:["Direct (",l,")"]})]})]}),e.jsxs("div",{className:"ca-cards ca-cards-bottom",children:[e.jsxs("div",{className:"ca-card ca-card-agentic",children:[e.jsx(i,{text:"Percentage of queries routed to the Agentic RAG pipeline ‚Äî an advanced multi-step search that automatically expands and retries when initial results are insufficient.",children:e.jsxs("span",{className:"ca-card-value",children:[c(o),"%"]})}),e.jsxs("span",{className:"ca-card-label",children:["Agentic RAG (",o,")"]})]}),e.jsxs("div",{className:"ca-card",children:[e.jsx(i,{text:"Average confidence score across all queries (0‚Äì100+). Score determines routing: <50 = Clarify, 50‚Äì74 = Agentic RAG, 75+ = Direct Answer. Factors: query specificity, error strings, RAG passage quality, engine version, multi-turn history.",children:e.jsx("span",{className:"ca-card-value",children:q})}),e.jsx("span",{className:"ca-card-label",children:"Avg Score"})]}),e.jsxs("div",{className:"ca-card",children:[e.jsx(i,{text:"Average character length of user queries. Longer queries tend to get higher confidence scores and better answers. Short queries (<30 chars) receive a -15 point penalty.",children:e.jsx("span",{className:"ca-card-value",children:A})}),e.jsx("span",{className:"ca-card-label",children:"Avg Query Length"})]})]}),e.jsxs("div",{className:"ca-distribution",children:[e.jsx("h4",{children:e.jsx(i,{text:"Visual breakdown of how queries were routed. Clarify (amber) = asked follow-up questions. Direct (green) = answered immediately. Agentic (purple) = used advanced multi-step search pipeline.",children:"Outcome Distribution"})}),e.jsxs("div",{className:"ca-bar",children:[n>0&&e.jsxs("div",{className:"ca-bar-segment ca-seg-clarify",style:{width:`${c(n)}%`},title:`Clarify: ${n} queries (${c(n)}%)`,children:[c(n),"%"]}),l>0&&e.jsxs("div",{className:"ca-bar-segment ca-seg-direct",style:{width:`${c(l)}%`},title:`Direct Answer: ${l} queries (${c(l)}%)`,children:[c(l),"%"]}),o>0&&e.jsxs("div",{className:"ca-bar-segment ca-seg-agentic",style:{width:`${c(o)}%`},title:`Agentic RAG: ${o} queries (${c(o)}%)`,children:[c(o),"%"]})]}),e.jsxs("div",{className:"ca-legend",children:[e.jsxs("span",{className:"ca-legend-item",children:[e.jsx("span",{className:"ca-dot ca-dot-clarify"})," Clarify"]}),e.jsxs("span",{className:"ca-legend-item",children:[e.jsx("span",{className:"ca-dot ca-dot-direct"})," Direct Answer"]}),e.jsxs("span",{className:"ca-legend-item",children:[e.jsx("span",{className:"ca-dot ca-dot-agentic"})," Agentic RAG"]})]})]}),N.length>0&&e.jsxs("div",{className:"ca-reasons",children:[e.jsx("h4",{children:e.jsx(i,{text:"Most frequent signals that influenced the confidence score. Each query can have multiple reasons. These help you understand WHY the AI chose to clarify vs answer directly.",children:"Top Scoring Reasons"})}),e.jsx("div",{className:"ca-reason-tags",children:N.map(([s,t])=>e.jsxs("span",{className:"ca-reason-tag",title:w(s),children:[s.replace(/_/g," ")," ",e.jsxs("strong",{children:["(",t,")"]})]},s))})]}),e.jsxs("div",{className:"ca-table-wrap",children:[e.jsx("h4",{children:e.jsx(i,{text:"Individual routing decisions in reverse chronological order. Each row shows one user query: when it happened, how it was routed, the confidence score, query length, which clarification round it was, and what signals influenced the decision.",children:"Recent Routing Decisions"})}),e.jsxs("table",{className:"ca-table",children:[e.jsx("thead",{children:e.jsxs("tr",{children:[e.jsx("th",{title:"When the query was submitted",children:"Time"}),e.jsx("th",{title:"How the AI routed this query: Clarify (asked follow-up), Direct (answered immediately), or Agentic (multi-step search)",children:"Outcome"}),e.jsx("th",{title:"Confidence score (0‚Äì100+). Higher = more confident. Thresholds: <50 Clarify, 50‚Äì74 Agentic, 75+ Direct",children:"Score"}),e.jsx("th",{title:"Character length of the user's query. Longer queries provide more context for better answers",children:"Query Len"}),e.jsx("th",{title:"Which clarification round this was. Round 0 = first query, Round 1+ = after follow-up questions",children:"Round"}),e.jsx("th",{title:"Signals that influenced the confidence score for this specific query",children:"Reasons"})]})}),e.jsx("tbody",{children:a.slice(0,25).map(s=>e.jsxs("tr",{className:`ca-row-${s.outcome}`,children:[e.jsx("td",{className:"ca-time",children:s.timestamp?s.timestamp.toLocaleString("en-US",{month:"short",day:"numeric",hour:"2-digit",minute:"2-digit"}):"‚Äî"}),e.jsx("td",{children:e.jsx("span",{className:`ca-outcome-badge ca-badge-${s.outcome}`,children:s.outcome==="clarify"?"üîç Clarify":s.outcome==="direct_answer"?"‚úÖ Direct":"ü§ñ Agentic"})}),e.jsx("td",{className:"ca-score",children:s.score??"‚Äî"}),e.jsx("td",{children:s.queryLength??"‚Äî"}),e.jsx("td",{children:s.round??s.clarifyRoundsCompleted??"‚Äî"}),e.jsx("td",{className:"ca-reasons-cell",children:(s.reasons||[]).map((t,x)=>e.jsx("span",{className:"ca-mini-tag",title:w(t),children:t.replace(/_/g," ")},x))})]},s.id))})]})]})]})]})}function w(a){const h={multiple_systems_identified:"The AI detected 2+ UE5 subsystems in the query (e.g., Lumen + Nanite). +30 points ‚Äî highly specific.",single_system_identified:"The AI detected exactly 1 UE5 subsystem. +15 points.",engine_version_provided:"The user specified their UE5 version (e.g., 5.3). +15 points ‚Äî helps target version-specific answers.",error_strings_provided:"The user included error messages or log output. +25 points ‚Äî very specific, high confidence.",platform_provided:"The user specified their platform (Windows, Mac, etc.). +5 points.",change_context_provided:"The user described what they changed recently. +10 points ‚Äî helps narrow root cause.",strong_rag_matches:"2+ high-quality transcript passages matched (>0.4 similarity). +25 points.",partial_rag_match:"1 high-quality transcript passage matched. +15 points.",decent_rag_matches:"2+ moderate transcript passages matched (0.35‚Äì0.40 similarity). +10 points.",short_query_penalty:"Query was under 30 characters. -15 points ‚Äî too vague for a direct answer.",no_structured_context_penalty:"No case report or multi-system info provided. -10 points."};if(a.startsWith("multi_turn_rounds_")){const u=a.split("_").pop();return`${u} clarification round(s) completed. +${Math.min(u*15,45)} points ‚Äî each round adds context.`}return h[a]||a.replace(/_/g," ")}export{Q as default};
