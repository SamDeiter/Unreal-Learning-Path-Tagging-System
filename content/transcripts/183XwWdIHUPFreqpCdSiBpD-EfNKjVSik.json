{
  "text": " So I mentioned right at the beginning of the presentation that we had some additional functionality that came in as a plugin. Essentially, Replication Graph is that thing. This is an additional plugin that has been introduced since 4.2 and gives us a new way to introduce additional layers of replication functionality. The key problem that Replication Graph solves is when you have a game that might be highly stateful, that might have hundreds of possible connections. Again, take something like a Fortnite, a Player Unknown Battlegrounds, Eve Online, any kind of MMO setup that might have a large amount of data to be considering, a large amount of connections to be considering this thing. That's going to come with a huge amount of network and CPU costs. We've already been through this initial flow. We've seen how the standard Unreal implementation works. It's quite powerful. There's quite a lot of ways that we can customize it. There's a lot of things that we can do to improve, reduce the amount that we're checking things, making sure that we're not always having to replicate over all the data all the time. Ultimately, even as part of that flow, even checking the dormancy, even checking relevancy, all of that comes with CPU costs. Even if we are saving further CPU costs down the line, there's still, for every actor in the level, for every actor relevant to a particular connection, there's still a lot of replicating that needs to be done. If we could take our replication, this is being done on every actor, possibly every frame, or these checks are at least being done very, very frequently, if not every frame. Then we can add in additional things that reduce some of those costs. Yes, every frame these checks are being done, but it's only every tenth of a second that we're actually checking this particular actor. Ultimately, when you see it as all possible replicated actors in the game all the time, that's a lot of checks being done. Even the fact that the game is having to reach out and find every replicated actor in the first place, this comes with a lot of CPU costs, as well as then the additional network cost. If we can break that flow up into something that's a little bit more persistent, where we can conceptualize data as a little bit more like, we're going to gather sections of relevancy together, we're going to maintain, we can move to almost a bit more of an event-based behavior, where it's only as and when information becomes relevant to a particular query that we can choose to include or exclude that data, that becomes a bit of a next logical conclusion. Rather than doing all these very global checks all the time about everything, we can group our data into more localized sets of checks to say, oh, let's just keep a permanent list of all the things that are always relevant. That way, we're not having to do get every actor and then check is this actor relevant. We've just got this permanent list of all the actors that are always relevant. We've got a permanent list of all the actors that are on this team and therefore always relevant to that team. We've got a permanent list of all the actors that are currently within this smaller section of the game. You can see, and then as and when those things change, those individual lists might change, there might be a small amount of replication change there, but ultimately the idea of this kind of data having more of a persistent representation, it not being a frame to frame, the entire thing gets binned off, the entire list gets reconstructed, you can see the benefits of that. That is exactly what replication graph essentially exists to introduce and that is the kind of problem that it solves. It can stand to reduce our CPU costs, it can reduce our network traffic if it's being smart, and it's essentially the whole system is made up of this concept of replication graph nodes and these are these kind of like actual object-based persistent data that will persist between frames on the server to kind of gather this data together and those nodes are individually responsible for kind of what they handle, what queries, what they're kind of there to solve, and the data that comes in and out of them. It's worth mentioning that replication graph is a kind of entirely native system so you can't leverage replication graph outside of C++, so it's no blueprint functionality here. So as I said, the kind of key components of the framework, we have the replication graph nodes. These nodes are responsible for building a list of actors to replicate to the client on demand. What's great about these is that their work can be split off across multiple frames so it doesn't have to take huge amounts of time. These nodes work great with persistent objects and it will reduce the amount of time spent gathering their lists. Like I said, we don't have to loop through every actor in the game first to even find out whether or not they replicate at all. Then the replication graph nodes are very much a tool. They provide a simple framework for us as programmers to leverage to meet our needs. They do kind of whatever we need them to do. With great power comes great responsibility. It's kind of up to us to define what they actually do for us. We can make nodes completely agnostic from our game. We can make them portable across several different projects or we can make them entirely game-specific, leveraging very, very specific bespoke game data. Lastly, our nodes let us control how and when a group of actors are actually updated to the client. They're not just responsible for gathering up the data, but they're actually responsible for whether or not this data goes to a particular client. It's the node that can get queried for whether or not we should even be replicating to a particular actor. This is kind of like a little brief demo explanation of how this graph might work. We can kind of see a couple of example nodes that we could possibly construct. Like I said, we might have a node that maintains a list of all the actors that are always relevant. Therefore, we don't need to constantly query all actors in the game to even maintain, to essentially go and regather the same actor. Our game state, for example, is the first obvious option. 50,000 different times across the whole running of our game, we can just have this as permanent data in our always relevant node. That way, we can just quickly query, hey, should this thing be, what does this node think should be replicated? Cool. It's just maintained this persistent list of all the things that are always relevant. Secondly, this other node, the node that is always relevant to the team. Possibly some of these are going to occur on a cross project basis. Some of these are going to be on a level by level basis. Maybe you're not doing a team-based game, so this might not be a node that you leverage in certain situations. But this node maintains the stuff that is always relevant to your team. This node maintains a list of everything that is either always dormant or always relevant. This node maintains a list of everything that is either always dormant or currently dormant or never dormant and therefore always needs to be replicated. This is the kind of node that can uniquely handle dormancy in a kind of abstract way. We might have a node that handles player states. Player states, unlike game states, might come in and out of relevancy depending on your distance for the player, but there might be unique data that is responsible for that. Therefore, maintaining that in its own list is something that makes quite a lot of sense. Then, for example, we could handle something like grid spatialization. Contrary to spatialization, if anyone is not familiar with those exact terms, we might break our game up into 1km by 1km grids and say that just to prevent a bunch of unnecessary distance checks, we can say that if something is in a 1km by 1km grid, obviously everything in that same grid will at least pass this relevancy check. Simultaneously, we could also say that anything in the nodes next to it are also relevant. If you are in this grid node or any of the nodes next to it, then you might be relevant. We could also maintain two lists of the data that is static, i.e. static world meshes like trees and benches. We know they're never going to go in and out based on gameplay responses, so we could have a static list of those and we could separately maintain a list of dynamic actors. Those are the things that might have a bit more event-based logic for this thing is leaving these bounds, they're entering these bounds, they've died, and so therefore they become irrelevant. Again, you can see the fact that we're just having to query data a lot less frequently, and when we're asking these nodes whether or not these things should be relevant, they've got a lot more data to hand. They're not having to go and call quite as many functions to even check whether or not this thing might be relevant right now. So the way that we can utilize a replication graph, it's worth mentioning also that replication graph can be used in conjunction with the previous. It's not necessary in an either-or situation, you can use it in conjunction. So how would we even introduce it? There's obviously two ways. We can do it through C++ and we can do it through the default engine any, and if we do it through our native code we can bind to our replication drivers, create a creation delegate, and then return our own replication graph through that lambda. The other way that we can do it is that we can specify this through the default engine config any, but the kind of important discrepancy between these two things is that when you're doing it through your default engine any, this is game wide. So if you have a replication, you may want to use replication graphs different depending on your gameplay scenario, maybe from level to level, then the default engine any doing it through your any class is not really going to be suitable. That's the C++ is where you're going to be wanting to do it if you maybe need to use a different replication graph in different scenarios.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 6.24,
      "text": " So I mentioned right at the beginning of the presentation that we had some additional functionality",
      "tokens": [
        50364,
        407,
        286,
        2835,
        558,
        412,
        264,
        2863,
        295,
        264,
        5860,
        300,
        321,
        632,
        512,
        4497,
        14980,
        50676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13716272660243659,
      "compression_ratio": 1.7511737089201878,
      "no_speech_prob": 0.1599830538034439
    },
    {
      "id": 1,
      "seek": 0,
      "start": 6.24,
      "end": 12.0,
      "text": " that came in as a plugin. Essentially, Replication Graph is that thing. This is an additional",
      "tokens": [
        50676,
        300,
        1361,
        294,
        382,
        257,
        23407,
        13,
        23596,
        11,
        1300,
        4770,
        399,
        21884,
        307,
        300,
        551,
        13,
        639,
        307,
        364,
        4497,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13716272660243659,
      "compression_ratio": 1.7511737089201878,
      "no_speech_prob": 0.1599830538034439
    },
    {
      "id": 2,
      "seek": 0,
      "start": 12.0,
      "end": 17.400000000000002,
      "text": " plugin that has been introduced since 4.2 and gives us a new way to introduce additional",
      "tokens": [
        50964,
        23407,
        300,
        575,
        668,
        7268,
        1670,
        1017,
        13,
        17,
        293,
        2709,
        505,
        257,
        777,
        636,
        281,
        5366,
        4497,
        51234
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13716272660243659,
      "compression_ratio": 1.7511737089201878,
      "no_speech_prob": 0.1599830538034439
    },
    {
      "id": 3,
      "seek": 0,
      "start": 17.400000000000002,
      "end": 26.0,
      "text": " layers of replication functionality. The key problem that Replication Graph solves is when",
      "tokens": [
        51234,
        7914,
        295,
        39911,
        14980,
        13,
        440,
        2141,
        1154,
        300,
        1300,
        4770,
        399,
        21884,
        39890,
        307,
        562,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13716272660243659,
      "compression_ratio": 1.7511737089201878,
      "no_speech_prob": 0.1599830538034439
    },
    {
      "id": 4,
      "seek": 2600,
      "start": 26.0,
      "end": 36.04,
      "text": " you have a game that might be highly stateful, that might have hundreds of possible connections.",
      "tokens": [
        50364,
        291,
        362,
        257,
        1216,
        300,
        1062,
        312,
        5405,
        1785,
        906,
        11,
        300,
        1062,
        362,
        6779,
        295,
        1944,
        9271,
        13,
        50866
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16741093722256747,
      "compression_ratio": 1.5138121546961325,
      "no_speech_prob": 0.5603129863739014
    },
    {
      "id": 5,
      "seek": 2600,
      "start": 36.04,
      "end": 47.96,
      "text": " Again, take something like a Fortnite, a Player Unknown Battlegrounds, Eve Online, any kind",
      "tokens": [
        50866,
        3764,
        11,
        747,
        746,
        411,
        257,
        28712,
        11,
        257,
        24920,
        32766,
        11846,
        2921,
        82,
        11,
        15544,
        16930,
        11,
        604,
        733,
        51462
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16741093722256747,
      "compression_ratio": 1.5138121546961325,
      "no_speech_prob": 0.5603129863739014
    },
    {
      "id": 6,
      "seek": 2600,
      "start": 47.96,
      "end": 53.56,
      "text": " of MMO setup that might have a large amount of data to be considering, a large amount",
      "tokens": [
        51462,
        295,
        376,
        18976,
        8657,
        300,
        1062,
        362,
        257,
        2416,
        2372,
        295,
        1412,
        281,
        312,
        8079,
        11,
        257,
        2416,
        2372,
        51742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16741093722256747,
      "compression_ratio": 1.5138121546961325,
      "no_speech_prob": 0.5603129863739014
    },
    {
      "id": 7,
      "seek": 5356,
      "start": 53.56,
      "end": 60.160000000000004,
      "text": " of connections to be considering this thing. That's going to come with a huge amount of",
      "tokens": [
        50364,
        295,
        9271,
        281,
        312,
        8079,
        341,
        551,
        13,
        663,
        311,
        516,
        281,
        808,
        365,
        257,
        2603,
        2372,
        295,
        50694
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11771023273468018,
      "compression_ratio": 1.71484375,
      "no_speech_prob": 0.37657031416893005
    },
    {
      "id": 8,
      "seek": 5356,
      "start": 60.160000000000004,
      "end": 65.60000000000001,
      "text": " network and CPU costs. We've already been through this initial flow. We've seen how",
      "tokens": [
        50694,
        3209,
        293,
        13199,
        5497,
        13,
        492,
        600,
        1217,
        668,
        807,
        341,
        5883,
        3095,
        13,
        492,
        600,
        1612,
        577,
        50966
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11771023273468018,
      "compression_ratio": 1.71484375,
      "no_speech_prob": 0.37657031416893005
    },
    {
      "id": 9,
      "seek": 5356,
      "start": 65.60000000000001,
      "end": 69.92,
      "text": " the standard Unreal implementation works. It's quite powerful. There's quite a lot of",
      "tokens": [
        50966,
        264,
        3832,
        34464,
        11420,
        1985,
        13,
        467,
        311,
        1596,
        4005,
        13,
        821,
        311,
        1596,
        257,
        688,
        295,
        51182
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11771023273468018,
      "compression_ratio": 1.71484375,
      "no_speech_prob": 0.37657031416893005
    },
    {
      "id": 10,
      "seek": 5356,
      "start": 69.92,
      "end": 76.76,
      "text": " ways that we can customize it. There's a lot of things that we can do to improve, reduce",
      "tokens": [
        51182,
        2098,
        300,
        321,
        393,
        19734,
        309,
        13,
        821,
        311,
        257,
        688,
        295,
        721,
        300,
        321,
        393,
        360,
        281,
        3470,
        11,
        5407,
        51524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11771023273468018,
      "compression_ratio": 1.71484375,
      "no_speech_prob": 0.37657031416893005
    },
    {
      "id": 11,
      "seek": 5356,
      "start": 76.76,
      "end": 80.96000000000001,
      "text": " the amount that we're checking things, making sure that we're not always having to replicate",
      "tokens": [
        51524,
        264,
        2372,
        300,
        321,
        434,
        8568,
        721,
        11,
        1455,
        988,
        300,
        321,
        434,
        406,
        1009,
        1419,
        281,
        25356,
        51734
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11771023273468018,
      "compression_ratio": 1.71484375,
      "no_speech_prob": 0.37657031416893005
    },
    {
      "id": 12,
      "seek": 8096,
      "start": 80.96,
      "end": 87.47999999999999,
      "text": " over all the data all the time. Ultimately, even as part of that flow, even checking the dormancy,",
      "tokens": [
        50364,
        670,
        439,
        264,
        1412,
        439,
        264,
        565,
        13,
        23921,
        11,
        754,
        382,
        644,
        295,
        300,
        3095,
        11,
        754,
        8568,
        264,
        12521,
        6717,
        11,
        50690
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16027955334595959,
      "compression_ratio": 1.8190045248868778,
      "no_speech_prob": 0.5684109926223755
    },
    {
      "id": 13,
      "seek": 8096,
      "start": 87.47999999999999,
      "end": 93.32,
      "text": " even checking relevancy, all of that comes with CPU costs. Even if we are saving further CPU costs",
      "tokens": [
        50690,
        754,
        8568,
        25916,
        6717,
        11,
        439,
        295,
        300,
        1487,
        365,
        13199,
        5497,
        13,
        2754,
        498,
        321,
        366,
        6816,
        3052,
        13199,
        5497,
        50982
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16027955334595959,
      "compression_ratio": 1.8190045248868778,
      "no_speech_prob": 0.5684109926223755
    },
    {
      "id": 14,
      "seek": 8096,
      "start": 93.32,
      "end": 100.03999999999999,
      "text": " down the line, there's still, for every actor in the level, for every actor relevant to a particular",
      "tokens": [
        50982,
        760,
        264,
        1622,
        11,
        456,
        311,
        920,
        11,
        337,
        633,
        8747,
        294,
        264,
        1496,
        11,
        337,
        633,
        8747,
        7340,
        281,
        257,
        1729,
        51318
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16027955334595959,
      "compression_ratio": 1.8190045248868778,
      "no_speech_prob": 0.5684109926223755
    },
    {
      "id": 15,
      "seek": 8096,
      "start": 100.03999999999999,
      "end": 107.63999999999999,
      "text": " connection, there's still a lot of replicating that needs to be done. If we could take our replication,",
      "tokens": [
        51318,
        4984,
        11,
        456,
        311,
        920,
        257,
        688,
        295,
        3248,
        30541,
        300,
        2203,
        281,
        312,
        1096,
        13,
        759,
        321,
        727,
        747,
        527,
        39911,
        11,
        51698
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16027955334595959,
      "compression_ratio": 1.8190045248868778,
      "no_speech_prob": 0.5684109926223755
    },
    {
      "id": 16,
      "seek": 10764,
      "start": 107.92,
      "end": 114.0,
      "text": " this is being done on every actor, possibly every frame, or these checks are at least being done",
      "tokens": [
        50378,
        341,
        307,
        885,
        1096,
        322,
        633,
        8747,
        11,
        6264,
        633,
        3920,
        11,
        420,
        613,
        13834,
        366,
        412,
        1935,
        885,
        1096,
        50682
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16513545824133832,
      "compression_ratio": 1.8458646616541354,
      "no_speech_prob": 0.029297690838575363
    },
    {
      "id": 17,
      "seek": 10764,
      "start": 114.0,
      "end": 119.52,
      "text": " very, very frequently, if not every frame. Then we can add in additional things that reduce some",
      "tokens": [
        50682,
        588,
        11,
        588,
        10374,
        11,
        498,
        406,
        633,
        3920,
        13,
        1396,
        321,
        393,
        909,
        294,
        4497,
        721,
        300,
        5407,
        512,
        50958
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16513545824133832,
      "compression_ratio": 1.8458646616541354,
      "no_speech_prob": 0.029297690838575363
    },
    {
      "id": 18,
      "seek": 10764,
      "start": 119.52,
      "end": 125.84,
      "text": " of those costs. Yes, every frame these checks are being done, but it's only every tenth of a second",
      "tokens": [
        50958,
        295,
        729,
        5497,
        13,
        1079,
        11,
        633,
        3920,
        613,
        13834,
        366,
        885,
        1096,
        11,
        457,
        309,
        311,
        787,
        633,
        27269,
        295,
        257,
        1150,
        51274
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16513545824133832,
      "compression_ratio": 1.8458646616541354,
      "no_speech_prob": 0.029297690838575363
    },
    {
      "id": 19,
      "seek": 10764,
      "start": 125.84,
      "end": 130.88,
      "text": " that we're actually checking this particular actor. Ultimately, when you see it as all possible",
      "tokens": [
        51274,
        300,
        321,
        434,
        767,
        8568,
        341,
        1729,
        8747,
        13,
        23921,
        11,
        562,
        291,
        536,
        309,
        382,
        439,
        1944,
        51526
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16513545824133832,
      "compression_ratio": 1.8458646616541354,
      "no_speech_prob": 0.029297690838575363
    },
    {
      "id": 20,
      "seek": 10764,
      "start": 130.88,
      "end": 137.28,
      "text": " replicated actors in the game all the time, that's a lot of checks being done. Even the fact that the",
      "tokens": [
        51526,
        46365,
        10037,
        294,
        264,
        1216,
        439,
        264,
        565,
        11,
        300,
        311,
        257,
        688,
        295,
        13834,
        885,
        1096,
        13,
        2754,
        264,
        1186,
        300,
        264,
        51846
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16513545824133832,
      "compression_ratio": 1.8458646616541354,
      "no_speech_prob": 0.029297690838575363
    },
    {
      "id": 21,
      "seek": 13764,
      "start": 138.07999999999998,
      "end": 142.95999999999998,
      "text": " game is having to reach out and find every replicated actor in the first place, this comes",
      "tokens": [
        50386,
        1216,
        307,
        1419,
        281,
        2524,
        484,
        293,
        915,
        633,
        46365,
        8747,
        294,
        264,
        700,
        1081,
        11,
        341,
        1487,
        50630
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10743642656990651,
      "compression_ratio": 1.6255506607929515,
      "no_speech_prob": 0.0003739992680493742
    },
    {
      "id": 22,
      "seek": 13764,
      "start": 142.95999999999998,
      "end": 150.07999999999998,
      "text": " with a lot of CPU costs, as well as then the additional network cost. If we can break that flow",
      "tokens": [
        50630,
        365,
        257,
        688,
        295,
        13199,
        5497,
        11,
        382,
        731,
        382,
        550,
        264,
        4497,
        3209,
        2063,
        13,
        759,
        321,
        393,
        1821,
        300,
        3095,
        50986
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10743642656990651,
      "compression_ratio": 1.6255506607929515,
      "no_speech_prob": 0.0003739992680493742
    },
    {
      "id": 23,
      "seek": 13764,
      "start": 150.07999999999998,
      "end": 157.11999999999998,
      "text": " up into something that's a little bit more persistent, where we can conceptualize data as",
      "tokens": [
        50986,
        493,
        666,
        746,
        300,
        311,
        257,
        707,
        857,
        544,
        24315,
        11,
        689,
        321,
        393,
        24106,
        1125,
        1412,
        382,
        51338
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10743642656990651,
      "compression_ratio": 1.6255506607929515,
      "no_speech_prob": 0.0003739992680493742
    },
    {
      "id": 24,
      "seek": 13764,
      "start": 157.11999999999998,
      "end": 163.67999999999998,
      "text": " a little bit more like, we're going to gather sections of relevancy together, we're going to",
      "tokens": [
        51338,
        257,
        707,
        857,
        544,
        411,
        11,
        321,
        434,
        516,
        281,
        5448,
        10863,
        295,
        25916,
        6717,
        1214,
        11,
        321,
        434,
        516,
        281,
        51666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10743642656990651,
      "compression_ratio": 1.6255506607929515,
      "no_speech_prob": 0.0003739992680493742
    },
    {
      "id": 25,
      "seek": 16368,
      "start": 163.76000000000002,
      "end": 171.12,
      "text": " maintain, we can move to almost a bit more of an event-based behavior, where it's only as and when",
      "tokens": [
        50368,
        6909,
        11,
        321,
        393,
        1286,
        281,
        1920,
        257,
        857,
        544,
        295,
        364,
        2280,
        12,
        6032,
        5223,
        11,
        689,
        309,
        311,
        787,
        382,
        293,
        562,
        50736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1193656497531467,
      "compression_ratio": 1.6582278481012658,
      "no_speech_prob": 0.011682633310556412
    },
    {
      "id": 26,
      "seek": 16368,
      "start": 172.0,
      "end": 178.08,
      "text": " information becomes relevant to a particular query that we can choose to include or exclude that data,",
      "tokens": [
        50780,
        1589,
        3643,
        7340,
        281,
        257,
        1729,
        14581,
        300,
        321,
        393,
        2826,
        281,
        4090,
        420,
        33536,
        300,
        1412,
        11,
        51084
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1193656497531467,
      "compression_ratio": 1.6582278481012658,
      "no_speech_prob": 0.011682633310556412
    },
    {
      "id": 27,
      "seek": 16368,
      "start": 179.28,
      "end": 184.96,
      "text": " that becomes a bit of a next logical conclusion. Rather than doing all these very global checks",
      "tokens": [
        51144,
        300,
        3643,
        257,
        857,
        295,
        257,
        958,
        14978,
        10063,
        13,
        16571,
        813,
        884,
        439,
        613,
        588,
        4338,
        13834,
        51428
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1193656497531467,
      "compression_ratio": 1.6582278481012658,
      "no_speech_prob": 0.011682633310556412
    },
    {
      "id": 28,
      "seek": 16368,
      "start": 184.96,
      "end": 191.12,
      "text": " all the time about everything, we can group our data into more localized sets of checks to say,",
      "tokens": [
        51428,
        439,
        264,
        565,
        466,
        1203,
        11,
        321,
        393,
        1594,
        527,
        1412,
        666,
        544,
        44574,
        6352,
        295,
        13834,
        281,
        584,
        11,
        51736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1193656497531467,
      "compression_ratio": 1.6582278481012658,
      "no_speech_prob": 0.011682633310556412
    },
    {
      "id": 29,
      "seek": 19112,
      "start": 191.12,
      "end": 197.76,
      "text": " oh, let's just keep a permanent list of all the things that are always relevant. That way,",
      "tokens": [
        50364,
        1954,
        11,
        718,
        311,
        445,
        1066,
        257,
        10996,
        1329,
        295,
        439,
        264,
        721,
        300,
        366,
        1009,
        7340,
        13,
        663,
        636,
        11,
        50696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11142923914153001,
      "compression_ratio": 2.2985781990521326,
      "no_speech_prob": 0.003375913016498089
    },
    {
      "id": 30,
      "seek": 19112,
      "start": 197.76,
      "end": 203.44,
      "text": " we're not having to do get every actor and then check is this actor relevant. We've just got this",
      "tokens": [
        50696,
        321,
        434,
        406,
        1419,
        281,
        360,
        483,
        633,
        8747,
        293,
        550,
        1520,
        307,
        341,
        8747,
        7340,
        13,
        492,
        600,
        445,
        658,
        341,
        50980
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11142923914153001,
      "compression_ratio": 2.2985781990521326,
      "no_speech_prob": 0.003375913016498089
    },
    {
      "id": 31,
      "seek": 19112,
      "start": 203.44,
      "end": 208.8,
      "text": " permanent list of all the actors that are always relevant. We've got a permanent list of all the",
      "tokens": [
        50980,
        10996,
        1329,
        295,
        439,
        264,
        10037,
        300,
        366,
        1009,
        7340,
        13,
        492,
        600,
        658,
        257,
        10996,
        1329,
        295,
        439,
        264,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11142923914153001,
      "compression_ratio": 2.2985781990521326,
      "no_speech_prob": 0.003375913016498089
    },
    {
      "id": 32,
      "seek": 19112,
      "start": 208.8,
      "end": 214.16,
      "text": " actors that are on this team and therefore always relevant to that team. We've got a permanent list",
      "tokens": [
        51248,
        10037,
        300,
        366,
        322,
        341,
        1469,
        293,
        4412,
        1009,
        7340,
        281,
        300,
        1469,
        13,
        492,
        600,
        658,
        257,
        10996,
        1329,
        51516
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11142923914153001,
      "compression_ratio": 2.2985781990521326,
      "no_speech_prob": 0.003375913016498089
    },
    {
      "id": 33,
      "seek": 19112,
      "start": 214.16,
      "end": 220.48000000000002,
      "text": " of all the actors that are currently within this smaller section of the game. You can see, and then",
      "tokens": [
        51516,
        295,
        439,
        264,
        10037,
        300,
        366,
        4362,
        1951,
        341,
        4356,
        3541,
        295,
        264,
        1216,
        13,
        509,
        393,
        536,
        11,
        293,
        550,
        51832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11142923914153001,
      "compression_ratio": 2.2985781990521326,
      "no_speech_prob": 0.003375913016498089
    },
    {
      "id": 34,
      "seek": 22048,
      "start": 220.88,
      "end": 225.6,
      "text": " as and when those things change, those individual lists might change, there might be a small amount",
      "tokens": [
        50384,
        382,
        293,
        562,
        729,
        721,
        1319,
        11,
        729,
        2609,
        14511,
        1062,
        1319,
        11,
        456,
        1062,
        312,
        257,
        1359,
        2372,
        50620
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08776900032970393,
      "compression_ratio": 1.8127340823970037,
      "no_speech_prob": 0.00016864563804119825
    },
    {
      "id": 35,
      "seek": 22048,
      "start": 225.6,
      "end": 231.44,
      "text": " of replication change there, but ultimately the idea of this kind of data having more of a",
      "tokens": [
        50620,
        295,
        39911,
        1319,
        456,
        11,
        457,
        6284,
        264,
        1558,
        295,
        341,
        733,
        295,
        1412,
        1419,
        544,
        295,
        257,
        50912
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08776900032970393,
      "compression_ratio": 1.8127340823970037,
      "no_speech_prob": 0.00016864563804119825
    },
    {
      "id": 36,
      "seek": 22048,
      "start": 231.44,
      "end": 237.12,
      "text": " persistent representation, it not being a frame to frame, the entire thing gets binned off, the",
      "tokens": [
        50912,
        24315,
        10290,
        11,
        309,
        406,
        885,
        257,
        3920,
        281,
        3920,
        11,
        264,
        2302,
        551,
        2170,
        5171,
        9232,
        766,
        11,
        264,
        51196
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08776900032970393,
      "compression_ratio": 1.8127340823970037,
      "no_speech_prob": 0.00016864563804119825
    },
    {
      "id": 37,
      "seek": 22048,
      "start": 237.12,
      "end": 243.2,
      "text": " entire list gets reconstructed, you can see the benefits of that. That is exactly what replication",
      "tokens": [
        51196,
        2302,
        1329,
        2170,
        31499,
        292,
        11,
        291,
        393,
        536,
        264,
        5311,
        295,
        300,
        13,
        663,
        307,
        2293,
        437,
        39911,
        51500
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08776900032970393,
      "compression_ratio": 1.8127340823970037,
      "no_speech_prob": 0.00016864563804119825
    },
    {
      "id": 38,
      "seek": 22048,
      "start": 243.2,
      "end": 248.79999999999998,
      "text": " graph essentially exists to introduce and that is the kind of problem that it solves. It can stand",
      "tokens": [
        51500,
        4295,
        4476,
        8198,
        281,
        5366,
        293,
        300,
        307,
        264,
        733,
        295,
        1154,
        300,
        309,
        39890,
        13,
        467,
        393,
        1463,
        51780
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08776900032970393,
      "compression_ratio": 1.8127340823970037,
      "no_speech_prob": 0.00016864563804119825
    },
    {
      "id": 39,
      "seek": 24880,
      "start": 248.8,
      "end": 255.44,
      "text": " to reduce our CPU costs, it can reduce our network traffic if it's being smart, and it's essentially",
      "tokens": [
        50364,
        281,
        5407,
        527,
        13199,
        5497,
        11,
        309,
        393,
        5407,
        527,
        3209,
        6419,
        498,
        309,
        311,
        885,
        4069,
        11,
        293,
        309,
        311,
        4476,
        50696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10927330879938035,
      "compression_ratio": 1.6638297872340426,
      "no_speech_prob": 0.01449637021869421
    },
    {
      "id": 40,
      "seek": 24880,
      "start": 255.44,
      "end": 260.32,
      "text": " the whole system is made up of this concept of replication graph nodes and these are these kind",
      "tokens": [
        50696,
        264,
        1379,
        1185,
        307,
        1027,
        493,
        295,
        341,
        3410,
        295,
        39911,
        4295,
        13891,
        293,
        613,
        366,
        613,
        733,
        50940
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10927330879938035,
      "compression_ratio": 1.6638297872340426,
      "no_speech_prob": 0.01449637021869421
    },
    {
      "id": 41,
      "seek": 24880,
      "start": 260.32,
      "end": 267.84000000000003,
      "text": " of like actual object-based persistent data that will persist between frames on the server to kind",
      "tokens": [
        50940,
        295,
        411,
        3539,
        2657,
        12,
        6032,
        24315,
        1412,
        300,
        486,
        13233,
        1296,
        12083,
        322,
        264,
        7154,
        281,
        733,
        51316
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10927330879938035,
      "compression_ratio": 1.6638297872340426,
      "no_speech_prob": 0.01449637021869421
    },
    {
      "id": 42,
      "seek": 24880,
      "start": 267.84000000000003,
      "end": 273.44,
      "text": " of gather this data together and those nodes are individually responsible for kind of what they",
      "tokens": [
        51316,
        295,
        5448,
        341,
        1412,
        1214,
        293,
        729,
        13891,
        366,
        16652,
        6250,
        337,
        733,
        295,
        437,
        436,
        51596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10927330879938035,
      "compression_ratio": 1.6638297872340426,
      "no_speech_prob": 0.01449637021869421
    },
    {
      "id": 43,
      "seek": 27344,
      "start": 273.44,
      "end": 278.71999999999997,
      "text": " handle, what queries, what they're kind of there to solve, and the data that comes in and out of",
      "tokens": [
        50364,
        4813,
        11,
        437,
        24109,
        11,
        437,
        436,
        434,
        733,
        295,
        456,
        281,
        5039,
        11,
        293,
        264,
        1412,
        300,
        1487,
        294,
        293,
        484,
        295,
        50628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10874338150024414,
      "compression_ratio": 1.7013574660633484,
      "no_speech_prob": 0.14012745022773743
    },
    {
      "id": 44,
      "seek": 27344,
      "start": 278.71999999999997,
      "end": 284.96,
      "text": " them. It's worth mentioning that replication graph is a kind of entirely native system so you can't",
      "tokens": [
        50628,
        552,
        13,
        467,
        311,
        3163,
        18315,
        300,
        39911,
        4295,
        307,
        257,
        733,
        295,
        7696,
        8470,
        1185,
        370,
        291,
        393,
        380,
        50940
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10874338150024414,
      "compression_ratio": 1.7013574660633484,
      "no_speech_prob": 0.14012745022773743
    },
    {
      "id": 45,
      "seek": 27344,
      "start": 284.96,
      "end": 291.52,
      "text": " leverage replication graph outside of C++, so it's no blueprint functionality here.",
      "tokens": [
        50940,
        13982,
        39911,
        4295,
        2380,
        295,
        383,
        25472,
        11,
        370,
        309,
        311,
        572,
        35868,
        14980,
        510,
        13,
        51268
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10874338150024414,
      "compression_ratio": 1.7013574660633484,
      "no_speech_prob": 0.14012745022773743
    },
    {
      "id": 46,
      "seek": 27344,
      "start": 293.2,
      "end": 299.76,
      "text": " So as I said, the kind of key components of the framework, we have the replication graph nodes.",
      "tokens": [
        51352,
        407,
        382,
        286,
        848,
        11,
        264,
        733,
        295,
        2141,
        6677,
        295,
        264,
        8388,
        11,
        321,
        362,
        264,
        39911,
        4295,
        13891,
        13,
        51680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10874338150024414,
      "compression_ratio": 1.7013574660633484,
      "no_speech_prob": 0.14012745022773743
    },
    {
      "id": 47,
      "seek": 29976,
      "start": 299.76,
      "end": 304.8,
      "text": " These nodes are responsible for building a list of actors to replicate to the client on demand.",
      "tokens": [
        50364,
        1981,
        13891,
        366,
        6250,
        337,
        2390,
        257,
        1329,
        295,
        10037,
        281,
        25356,
        281,
        264,
        6423,
        322,
        4733,
        13,
        50616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06480788317593661,
      "compression_ratio": 1.7473309608540926,
      "no_speech_prob": 0.008573358878493309
    },
    {
      "id": 48,
      "seek": 29976,
      "start": 305.76,
      "end": 309.84,
      "text": " What's great about these is that their work can be split off across multiple frames so it doesn't",
      "tokens": [
        50664,
        708,
        311,
        869,
        466,
        613,
        307,
        300,
        641,
        589,
        393,
        312,
        7472,
        766,
        2108,
        3866,
        12083,
        370,
        309,
        1177,
        380,
        50868
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06480788317593661,
      "compression_ratio": 1.7473309608540926,
      "no_speech_prob": 0.008573358878493309
    },
    {
      "id": 49,
      "seek": 29976,
      "start": 309.84,
      "end": 314.48,
      "text": " have to take huge amounts of time. These nodes work great with persistent objects and it will reduce",
      "tokens": [
        50868,
        362,
        281,
        747,
        2603,
        11663,
        295,
        565,
        13,
        1981,
        13891,
        589,
        869,
        365,
        24315,
        6565,
        293,
        309,
        486,
        5407,
        51100
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06480788317593661,
      "compression_ratio": 1.7473309608540926,
      "no_speech_prob": 0.008573358878493309
    },
    {
      "id": 50,
      "seek": 29976,
      "start": 314.48,
      "end": 318.08,
      "text": " the amount of time spent gathering their lists. Like I said, we don't have to loop through every",
      "tokens": [
        51100,
        264,
        2372,
        295,
        565,
        4418,
        13519,
        641,
        14511,
        13,
        1743,
        286,
        848,
        11,
        321,
        500,
        380,
        362,
        281,
        6367,
        807,
        633,
        51280
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06480788317593661,
      "compression_ratio": 1.7473309608540926,
      "no_speech_prob": 0.008573358878493309
    },
    {
      "id": 51,
      "seek": 29976,
      "start": 318.08,
      "end": 325.76,
      "text": " actor in the game first to even find out whether or not they replicate at all. Then the replication",
      "tokens": [
        51280,
        8747,
        294,
        264,
        1216,
        700,
        281,
        754,
        915,
        484,
        1968,
        420,
        406,
        436,
        25356,
        412,
        439,
        13,
        1396,
        264,
        39911,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06480788317593661,
      "compression_ratio": 1.7473309608540926,
      "no_speech_prob": 0.008573358878493309
    },
    {
      "id": 52,
      "seek": 32576,
      "start": 325.76,
      "end": 331.03999999999996,
      "text": " graph nodes are very much a tool. They provide a simple framework for us as programmers to leverage",
      "tokens": [
        50364,
        4295,
        13891,
        366,
        588,
        709,
        257,
        2290,
        13,
        814,
        2893,
        257,
        2199,
        8388,
        337,
        505,
        382,
        41504,
        281,
        13982,
        50628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07956371651039468,
      "compression_ratio": 1.7472527472527473,
      "no_speech_prob": 0.132675901055336
    },
    {
      "id": 53,
      "seek": 32576,
      "start": 331.03999999999996,
      "end": 338.08,
      "text": " to meet our needs. They do kind of whatever we need them to do. With great power comes great",
      "tokens": [
        50628,
        281,
        1677,
        527,
        2203,
        13,
        814,
        360,
        733,
        295,
        2035,
        321,
        643,
        552,
        281,
        360,
        13,
        2022,
        869,
        1347,
        1487,
        869,
        50980
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07956371651039468,
      "compression_ratio": 1.7472527472527473,
      "no_speech_prob": 0.132675901055336
    },
    {
      "id": 54,
      "seek": 32576,
      "start": 338.08,
      "end": 343.76,
      "text": " responsibility. It's kind of up to us to define what they actually do for us. We can make nodes",
      "tokens": [
        50980,
        6357,
        13,
        467,
        311,
        733,
        295,
        493,
        281,
        505,
        281,
        6964,
        437,
        436,
        767,
        360,
        337,
        505,
        13,
        492,
        393,
        652,
        13891,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07956371651039468,
      "compression_ratio": 1.7472527472527473,
      "no_speech_prob": 0.132675901055336
    },
    {
      "id": 55,
      "seek": 32576,
      "start": 343.76,
      "end": 347.76,
      "text": " completely agnostic from our game. We can make them portable across several different projects",
      "tokens": [
        51264,
        2584,
        623,
        77,
        19634,
        490,
        527,
        1216,
        13,
        492,
        393,
        652,
        552,
        21800,
        2108,
        2940,
        819,
        4455,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07956371651039468,
      "compression_ratio": 1.7472527472527473,
      "no_speech_prob": 0.132675901055336
    },
    {
      "id": 56,
      "seek": 32576,
      "start": 347.76,
      "end": 352.71999999999997,
      "text": " or we can make them entirely game-specific, leveraging very, very specific bespoke game data.",
      "tokens": [
        51464,
        420,
        321,
        393,
        652,
        552,
        7696,
        1216,
        12,
        29258,
        11,
        32666,
        588,
        11,
        588,
        2685,
        4097,
        48776,
        1216,
        1412,
        13,
        51712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07956371651039468,
      "compression_ratio": 1.7472527472527473,
      "no_speech_prob": 0.132675901055336
    },
    {
      "id": 57,
      "seek": 35272,
      "start": 352.72,
      "end": 357.68,
      "text": " Lastly, our nodes let us control how and when a group of actors are actually updated to the client.",
      "tokens": [
        50364,
        18072,
        11,
        527,
        13891,
        718,
        505,
        1969,
        577,
        293,
        562,
        257,
        1594,
        295,
        10037,
        366,
        767,
        10588,
        281,
        264,
        6423,
        13,
        50612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2495575325161803,
      "compression_ratio": 1.7233201581027668,
      "no_speech_prob": 0.0013248419854789972
    },
    {
      "id": 58,
      "seek": 35272,
      "start": 357.68,
      "end": 362.08000000000004,
      "text": " They're not just responsible for gathering up the data, but they're actually responsible for",
      "tokens": [
        50612,
        814,
        434,
        406,
        445,
        6250,
        337,
        13519,
        493,
        264,
        1412,
        11,
        457,
        436,
        434,
        767,
        6250,
        337,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2495575325161803,
      "compression_ratio": 1.7233201581027668,
      "no_speech_prob": 0.0013248419854789972
    },
    {
      "id": 59,
      "seek": 35272,
      "start": 362.08000000000004,
      "end": 366.24,
      "text": " whether or not this data goes to a particular client. It's the node that can get queried",
      "tokens": [
        50832,
        1968,
        420,
        406,
        341,
        1412,
        1709,
        281,
        257,
        1729,
        6423,
        13,
        467,
        311,
        264,
        9984,
        300,
        393,
        483,
        7083,
        1091,
        51040
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2495575325161803,
      "compression_ratio": 1.7233201581027668,
      "no_speech_prob": 0.0013248419854789972
    },
    {
      "id": 60,
      "seek": 35272,
      "start": 366.24,
      "end": 369.76000000000005,
      "text": " for whether or not we should even be replicating to a particular actor.",
      "tokens": [
        51040,
        337,
        1968,
        420,
        406,
        321,
        820,
        754,
        312,
        3248,
        30541,
        281,
        257,
        1729,
        8747,
        13,
        51216
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2495575325161803,
      "compression_ratio": 1.7233201581027668,
      "no_speech_prob": 0.0013248419854789972
    },
    {
      "id": 61,
      "seek": 35272,
      "start": 371.28000000000003,
      "end": 377.20000000000005,
      "text": " This is kind of like a little brief demo explanation of how this graph might work.",
      "tokens": [
        51292,
        639,
        307,
        733,
        295,
        411,
        257,
        707,
        5353,
        10723,
        10835,
        295,
        577,
        341,
        4295,
        1062,
        589,
        13,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2495575325161803,
      "compression_ratio": 1.7233201581027668,
      "no_speech_prob": 0.0013248419854789972
    },
    {
      "id": 62,
      "seek": 37720,
      "start": 378.15999999999997,
      "end": 382.71999999999997,
      "text": " We can kind of see a couple of example nodes that we could possibly construct.",
      "tokens": [
        50412,
        492,
        393,
        733,
        295,
        536,
        257,
        1916,
        295,
        1365,
        13891,
        300,
        321,
        727,
        6264,
        7690,
        13,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26733467016327245,
      "compression_ratio": 1.618421052631579,
      "no_speech_prob": 0.10365860164165497
    },
    {
      "id": 63,
      "seek": 37720,
      "start": 383.92,
      "end": 389.59999999999997,
      "text": " Like I said, we might have a node that maintains a list of all the actors that are always relevant.",
      "tokens": [
        50700,
        1743,
        286,
        848,
        11,
        321,
        1062,
        362,
        257,
        9984,
        300,
        33385,
        257,
        1329,
        295,
        439,
        264,
        10037,
        300,
        366,
        1009,
        7340,
        13,
        50984
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26733467016327245,
      "compression_ratio": 1.618421052631579,
      "no_speech_prob": 0.10365860164165497
    },
    {
      "id": 64,
      "seek": 37720,
      "start": 389.59999999999997,
      "end": 395.12,
      "text": " Therefore, we don't need to constantly query all actors in the game to even maintain,",
      "tokens": [
        50984,
        7504,
        11,
        321,
        500,
        380,
        643,
        281,
        6460,
        14581,
        439,
        10037,
        294,
        264,
        1216,
        281,
        754,
        6909,
        11,
        51260
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26733467016327245,
      "compression_ratio": 1.618421052631579,
      "no_speech_prob": 0.10365860164165497
    },
    {
      "id": 65,
      "seek": 37720,
      "start": 395.12,
      "end": 401.12,
      "text": " to essentially go and regather the same actor. Our game state, for example, is the first obvious option.",
      "tokens": [
        51260,
        281,
        4476,
        352,
        293,
        1121,
        1172,
        264,
        912,
        8747,
        13,
        2621,
        1216,
        1785,
        11,
        337,
        1365,
        11,
        307,
        264,
        700,
        6322,
        3614,
        13,
        51560
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26733467016327245,
      "compression_ratio": 1.618421052631579,
      "no_speech_prob": 0.10365860164165497
    },
    {
      "id": 66,
      "seek": 40112,
      "start": 401.28000000000003,
      "end": 405.84000000000003,
      "text": " 50,000 different times across the whole running of our game, we can just have this as permanent data",
      "tokens": [
        50372,
        2625,
        11,
        1360,
        819,
        1413,
        2108,
        264,
        1379,
        2614,
        295,
        527,
        1216,
        11,
        321,
        393,
        445,
        362,
        341,
        382,
        10996,
        1412,
        50600
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3132271236843533,
      "compression_ratio": 1.6535087719298245,
      "no_speech_prob": 0.0874861404299736
    },
    {
      "id": 67,
      "seek": 40112,
      "start": 405.84000000000003,
      "end": 412.4,
      "text": " in our always relevant node. That way, we can just quickly query, hey, should this thing be,",
      "tokens": [
        50600,
        294,
        527,
        1009,
        7340,
        9984,
        13,
        663,
        636,
        11,
        321,
        393,
        445,
        2661,
        14581,
        11,
        4177,
        11,
        820,
        341,
        551,
        312,
        11,
        50928
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3132271236843533,
      "compression_ratio": 1.6535087719298245,
      "no_speech_prob": 0.0874861404299736
    },
    {
      "id": 68,
      "seek": 40112,
      "start": 413.28000000000003,
      "end": 418.32,
      "text": " what does this node think should be replicated? Cool. It's just maintained this persistent list",
      "tokens": [
        50972,
        437,
        775,
        341,
        9984,
        519,
        820,
        312,
        46365,
        30,
        8561,
        13,
        467,
        311,
        445,
        17578,
        341,
        24315,
        1329,
        51224
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3132271236843533,
      "compression_ratio": 1.6535087719298245,
      "no_speech_prob": 0.0874861404299736
    },
    {
      "id": 69,
      "seek": 40112,
      "start": 418.32,
      "end": 426.0,
      "text": " of all the things that are always relevant. Secondly, this other node, the node that is",
      "tokens": [
        51224,
        295,
        439,
        264,
        721,
        300,
        366,
        1009,
        7340,
        13,
        19483,
        11,
        341,
        661,
        9984,
        11,
        264,
        9984,
        300,
        307,
        51608
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3132271236843533,
      "compression_ratio": 1.6535087719298245,
      "no_speech_prob": 0.0874861404299736
    },
    {
      "id": 70,
      "seek": 42600,
      "start": 426.96,
      "end": 433.68,
      "text": " always relevant to the team. Possibly some of these are going to occur on a cross project basis.",
      "tokens": [
        50412,
        1009,
        7340,
        281,
        264,
        1469,
        13,
        33112,
        3545,
        512,
        295,
        613,
        366,
        516,
        281,
        5160,
        322,
        257,
        3278,
        1716,
        5143,
        13,
        50748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2954298593465564,
      "compression_ratio": 1.8893805309734513,
      "no_speech_prob": 0.11910289525985718
    },
    {
      "id": 71,
      "seek": 42600,
      "start": 433.68,
      "end": 437.92,
      "text": " Some of these are going to be on a level by level basis. Maybe you're not doing a team-based game,",
      "tokens": [
        50748,
        2188,
        295,
        613,
        366,
        516,
        281,
        312,
        322,
        257,
        1496,
        538,
        1496,
        5143,
        13,
        2704,
        291,
        434,
        406,
        884,
        257,
        1469,
        12,
        6032,
        1216,
        11,
        50960
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2954298593465564,
      "compression_ratio": 1.8893805309734513,
      "no_speech_prob": 0.11910289525985718
    },
    {
      "id": 72,
      "seek": 42600,
      "start": 437.92,
      "end": 441.36,
      "text": " so this might not be a node that you leverage in certain situations.",
      "tokens": [
        50960,
        370,
        341,
        1062,
        406,
        312,
        257,
        9984,
        300,
        291,
        13982,
        294,
        1629,
        6851,
        13,
        51132
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2954298593465564,
      "compression_ratio": 1.8893805309734513,
      "no_speech_prob": 0.11910289525985718
    },
    {
      "id": 73,
      "seek": 42600,
      "start": 442.16,
      "end": 445.52,
      "text": " But this node maintains the stuff that is always relevant to your team.",
      "tokens": [
        51172,
        583,
        341,
        9984,
        33385,
        264,
        1507,
        300,
        307,
        1009,
        7340,
        281,
        428,
        1469,
        13,
        51340
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2954298593465564,
      "compression_ratio": 1.8893805309734513,
      "no_speech_prob": 0.11910289525985718
    },
    {
      "id": 74,
      "seek": 42600,
      "start": 445.52,
      "end": 451.84,
      "text": " This node maintains a list of everything that is either always dormant or always relevant.",
      "tokens": [
        51340,
        639,
        9984,
        33385,
        257,
        1329,
        295,
        1203,
        300,
        307,
        2139,
        1009,
        12521,
        394,
        420,
        1009,
        7340,
        13,
        51656
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2954298593465564,
      "compression_ratio": 1.8893805309734513,
      "no_speech_prob": 0.11910289525985718
    },
    {
      "id": 75,
      "seek": 45184,
      "start": 451.84,
      "end": 458.23999999999995,
      "text": " This node maintains a list of everything that is either always dormant or currently dormant",
      "tokens": [
        50364,
        639,
        9984,
        33385,
        257,
        1329,
        295,
        1203,
        300,
        307,
        2139,
        1009,
        12521,
        394,
        420,
        4362,
        12521,
        394,
        50684
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12391790051326573,
      "compression_ratio": 1.8404669260700388,
      "no_speech_prob": 0.03731108829379082
    },
    {
      "id": 76,
      "seek": 45184,
      "start": 458.23999999999995,
      "end": 463.59999999999997,
      "text": " or never dormant and therefore always needs to be replicated. This is the kind of node that can",
      "tokens": [
        50684,
        420,
        1128,
        12521,
        394,
        293,
        4412,
        1009,
        2203,
        281,
        312,
        46365,
        13,
        639,
        307,
        264,
        733,
        295,
        9984,
        300,
        393,
        50952
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12391790051326573,
      "compression_ratio": 1.8404669260700388,
      "no_speech_prob": 0.03731108829379082
    },
    {
      "id": 77,
      "seek": 45184,
      "start": 463.59999999999997,
      "end": 470.32,
      "text": " uniquely handle dormancy in a kind of abstract way. We might have a node that handles player states.",
      "tokens": [
        50952,
        31474,
        4813,
        12521,
        6717,
        294,
        257,
        733,
        295,
        12649,
        636,
        13,
        492,
        1062,
        362,
        257,
        9984,
        300,
        18722,
        4256,
        4368,
        13,
        51288
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12391790051326573,
      "compression_ratio": 1.8404669260700388,
      "no_speech_prob": 0.03731108829379082
    },
    {
      "id": 78,
      "seek": 45184,
      "start": 470.32,
      "end": 475.59999999999997,
      "text": " Player states, unlike game states, might come in and out of relevancy depending on your distance",
      "tokens": [
        51288,
        24920,
        4368,
        11,
        8343,
        1216,
        4368,
        11,
        1062,
        808,
        294,
        293,
        484,
        295,
        25916,
        6717,
        5413,
        322,
        428,
        4560,
        51552
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12391790051326573,
      "compression_ratio": 1.8404669260700388,
      "no_speech_prob": 0.03731108829379082
    },
    {
      "id": 79,
      "seek": 45184,
      "start": 475.59999999999997,
      "end": 480.4,
      "text": " for the player, but there might be unique data that is responsible for that. Therefore,",
      "tokens": [
        51552,
        337,
        264,
        4256,
        11,
        457,
        456,
        1062,
        312,
        3845,
        1412,
        300,
        307,
        6250,
        337,
        300,
        13,
        7504,
        11,
        51792
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12391790051326573,
      "compression_ratio": 1.8404669260700388,
      "no_speech_prob": 0.03731108829379082
    },
    {
      "id": 80,
      "seek": 48040,
      "start": 480.4,
      "end": 483.59999999999997,
      "text": " maintaining that in its own list is something that makes quite a lot of sense.",
      "tokens": [
        50364,
        14916,
        300,
        294,
        1080,
        1065,
        1329,
        307,
        746,
        300,
        1669,
        1596,
        257,
        688,
        295,
        2020,
        13,
        50524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15161246879428042,
      "compression_ratio": 1.6835443037974684,
      "no_speech_prob": 0.0032724468037486076
    },
    {
      "id": 81,
      "seek": 48040,
      "start": 484.56,
      "end": 488.15999999999997,
      "text": " Then, for example, we could handle something like grid spatialization.",
      "tokens": [
        50572,
        1396,
        11,
        337,
        1365,
        11,
        321,
        727,
        4813,
        746,
        411,
        10748,
        23598,
        2144,
        13,
        50752
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15161246879428042,
      "compression_ratio": 1.6835443037974684,
      "no_speech_prob": 0.0032724468037486076
    },
    {
      "id": 82,
      "seek": 48040,
      "start": 490.47999999999996,
      "end": 494.4,
      "text": " Contrary to spatialization, if anyone is not familiar with those exact terms,",
      "tokens": [
        50868,
        4839,
        81,
        822,
        281,
        23598,
        2144,
        11,
        498,
        2878,
        307,
        406,
        4963,
        365,
        729,
        1900,
        2115,
        11,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15161246879428042,
      "compression_ratio": 1.6835443037974684,
      "no_speech_prob": 0.0032724468037486076
    },
    {
      "id": 83,
      "seek": 48040,
      "start": 494.4,
      "end": 502.64,
      "text": " we might break our game up into 1km by 1km grids and say that just to prevent a bunch of",
      "tokens": [
        51064,
        321,
        1062,
        1821,
        527,
        1216,
        493,
        666,
        502,
        17950,
        538,
        502,
        17950,
        677,
        3742,
        293,
        584,
        300,
        445,
        281,
        4871,
        257,
        3840,
        295,
        51476
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15161246879428042,
      "compression_ratio": 1.6835443037974684,
      "no_speech_prob": 0.0032724468037486076
    },
    {
      "id": 84,
      "seek": 48040,
      "start": 502.64,
      "end": 509.44,
      "text": " unnecessary distance checks, we can say that if something is in a 1km by 1km grid,",
      "tokens": [
        51476,
        19350,
        4560,
        13834,
        11,
        321,
        393,
        584,
        300,
        498,
        746,
        307,
        294,
        257,
        502,
        17950,
        538,
        502,
        17950,
        10748,
        11,
        51816
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15161246879428042,
      "compression_ratio": 1.6835443037974684,
      "no_speech_prob": 0.0032724468037486076
    },
    {
      "id": 85,
      "seek": 50944,
      "start": 509.44,
      "end": 513.2,
      "text": " obviously everything in that same grid will at least pass this relevancy check.",
      "tokens": [
        50364,
        2745,
        1203,
        294,
        300,
        912,
        10748,
        486,
        412,
        1935,
        1320,
        341,
        25916,
        6717,
        1520,
        13,
        50552
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11868900182295819,
      "compression_ratio": 1.7660550458715596,
      "no_speech_prob": 0.0003150140692014247
    },
    {
      "id": 86,
      "seek": 50944,
      "start": 515.76,
      "end": 521.52,
      "text": " Simultaneously, we could also say that anything in the nodes next to it are also relevant. If you are",
      "tokens": [
        50680,
        3998,
        723,
        13131,
        11,
        321,
        727,
        611,
        584,
        300,
        1340,
        294,
        264,
        13891,
        958,
        281,
        309,
        366,
        611,
        7340,
        13,
        759,
        291,
        366,
        50968
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11868900182295819,
      "compression_ratio": 1.7660550458715596,
      "no_speech_prob": 0.0003150140692014247
    },
    {
      "id": 87,
      "seek": 50944,
      "start": 521.52,
      "end": 527.2,
      "text": " in this grid node or any of the nodes next to it, then you might be relevant. We could also maintain",
      "tokens": [
        50968,
        294,
        341,
        10748,
        9984,
        420,
        604,
        295,
        264,
        13891,
        958,
        281,
        309,
        11,
        550,
        291,
        1062,
        312,
        7340,
        13,
        492,
        727,
        611,
        6909,
        51252
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11868900182295819,
      "compression_ratio": 1.7660550458715596,
      "no_speech_prob": 0.0003150140692014247
    },
    {
      "id": 88,
      "seek": 50944,
      "start": 527.2,
      "end": 534.88,
      "text": " two lists of the data that is static, i.e. static world meshes like trees and benches. We know they're",
      "tokens": [
        51252,
        732,
        14511,
        295,
        264,
        1412,
        300,
        307,
        13437,
        11,
        741,
        13,
        68,
        13,
        13437,
        1002,
        3813,
        8076,
        411,
        5852,
        293,
        3271,
        3781,
        13,
        492,
        458,
        436,
        434,
        51636
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11868900182295819,
      "compression_ratio": 1.7660550458715596,
      "no_speech_prob": 0.0003150140692014247
    },
    {
      "id": 89,
      "seek": 53488,
      "start": 534.88,
      "end": 540.32,
      "text": " never going to go in and out based on gameplay responses, so we could have a static list of",
      "tokens": [
        50364,
        1128,
        516,
        281,
        352,
        294,
        293,
        484,
        2361,
        322,
        11421,
        13019,
        11,
        370,
        321,
        727,
        362,
        257,
        13437,
        1329,
        295,
        50636
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10663147310240079,
      "compression_ratio": 1.7830882352941178,
      "no_speech_prob": 0.0533590093255043
    },
    {
      "id": 90,
      "seek": 53488,
      "start": 540.32,
      "end": 545.36,
      "text": " those and we could separately maintain a list of dynamic actors. Those are the things that might",
      "tokens": [
        50636,
        729,
        293,
        321,
        727,
        14759,
        6909,
        257,
        1329,
        295,
        8546,
        10037,
        13,
        3950,
        366,
        264,
        721,
        300,
        1062,
        50888
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10663147310240079,
      "compression_ratio": 1.7830882352941178,
      "no_speech_prob": 0.0533590093255043
    },
    {
      "id": 91,
      "seek": 53488,
      "start": 545.36,
      "end": 550.32,
      "text": " have a bit more event-based logic for this thing is leaving these bounds, they're entering these",
      "tokens": [
        50888,
        362,
        257,
        857,
        544,
        2280,
        12,
        6032,
        9952,
        337,
        341,
        551,
        307,
        5012,
        613,
        29905,
        11,
        436,
        434,
        11104,
        613,
        51136
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10663147310240079,
      "compression_ratio": 1.7830882352941178,
      "no_speech_prob": 0.0533590093255043
    },
    {
      "id": 92,
      "seek": 53488,
      "start": 550.32,
      "end": 557.28,
      "text": " bounds, they've died, and so therefore they become irrelevant. Again, you can see the fact that we're",
      "tokens": [
        51136,
        29905,
        11,
        436,
        600,
        4539,
        11,
        293,
        370,
        4412,
        436,
        1813,
        28682,
        13,
        3764,
        11,
        291,
        393,
        536,
        264,
        1186,
        300,
        321,
        434,
        51484
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10663147310240079,
      "compression_ratio": 1.7830882352941178,
      "no_speech_prob": 0.0533590093255043
    },
    {
      "id": 93,
      "seek": 53488,
      "start": 557.28,
      "end": 563.04,
      "text": " just having to query data a lot less frequently, and when we're asking these nodes whether or not",
      "tokens": [
        51484,
        445,
        1419,
        281,
        14581,
        1412,
        257,
        688,
        1570,
        10374,
        11,
        293,
        562,
        321,
        434,
        3365,
        613,
        13891,
        1968,
        420,
        406,
        51772
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10663147310240079,
      "compression_ratio": 1.7830882352941178,
      "no_speech_prob": 0.0533590093255043
    },
    {
      "id": 94,
      "seek": 56304,
      "start": 563.04,
      "end": 567.5999999999999,
      "text": " these things should be relevant, they've got a lot more data to hand. They're not having to go and",
      "tokens": [
        50364,
        613,
        721,
        820,
        312,
        7340,
        11,
        436,
        600,
        658,
        257,
        688,
        544,
        1412,
        281,
        1011,
        13,
        814,
        434,
        406,
        1419,
        281,
        352,
        293,
        50592
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1034265140007282,
      "compression_ratio": 1.7228070175438597,
      "no_speech_prob": 0.006094773765653372
    },
    {
      "id": 95,
      "seek": 56304,
      "start": 567.5999999999999,
      "end": 571.5999999999999,
      "text": " call quite as many functions to even check whether or not this thing might be relevant right now.",
      "tokens": [
        50592,
        818,
        1596,
        382,
        867,
        6828,
        281,
        754,
        1520,
        1968,
        420,
        406,
        341,
        551,
        1062,
        312,
        7340,
        558,
        586,
        13,
        50792
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1034265140007282,
      "compression_ratio": 1.7228070175438597,
      "no_speech_prob": 0.006094773765653372
    },
    {
      "id": 96,
      "seek": 56304,
      "start": 573.1999999999999,
      "end": 577.28,
      "text": " So the way that we can utilize a replication graph, it's worth mentioning also that replication",
      "tokens": [
        50872,
        407,
        264,
        636,
        300,
        321,
        393,
        16117,
        257,
        39911,
        4295,
        11,
        309,
        311,
        3163,
        18315,
        611,
        300,
        39911,
        51076
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1034265140007282,
      "compression_ratio": 1.7228070175438597,
      "no_speech_prob": 0.006094773765653372
    },
    {
      "id": 97,
      "seek": 56304,
      "start": 577.28,
      "end": 582.7199999999999,
      "text": " graph can be used in conjunction with the previous. It's not necessary in an either-or situation,",
      "tokens": [
        51076,
        4295,
        393,
        312,
        1143,
        294,
        27482,
        365,
        264,
        3894,
        13,
        467,
        311,
        406,
        4818,
        294,
        364,
        2139,
        12,
        284,
        2590,
        11,
        51348
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1034265140007282,
      "compression_ratio": 1.7228070175438597,
      "no_speech_prob": 0.006094773765653372
    },
    {
      "id": 98,
      "seek": 56304,
      "start": 582.7199999999999,
      "end": 588.56,
      "text": " you can use it in conjunction. So how would we even introduce it? There's obviously two ways. We can",
      "tokens": [
        51348,
        291,
        393,
        764,
        309,
        294,
        27482,
        13,
        407,
        577,
        576,
        321,
        754,
        5366,
        309,
        30,
        821,
        311,
        2745,
        732,
        2098,
        13,
        492,
        393,
        51640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1034265140007282,
      "compression_ratio": 1.7228070175438597,
      "no_speech_prob": 0.006094773765653372
    },
    {
      "id": 99,
      "seek": 58856,
      "start": 588.56,
      "end": 596.0,
      "text": " do it through C++ and we can do it through the default engine any, and if we do it through our",
      "tokens": [
        50364,
        360,
        309,
        807,
        383,
        25472,
        293,
        321,
        393,
        360,
        309,
        807,
        264,
        7576,
        2848,
        604,
        11,
        293,
        498,
        321,
        360,
        309,
        807,
        527,
        50736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10524743004182799,
      "compression_ratio": 2.00836820083682,
      "no_speech_prob": 0.12923970818519592
    },
    {
      "id": 100,
      "seek": 58856,
      "start": 596.0,
      "end": 601.8399999999999,
      "text": " native code we can bind to our replication drivers, create a creation delegate, and then return",
      "tokens": [
        50736,
        8470,
        3089,
        321,
        393,
        14786,
        281,
        527,
        39911,
        11590,
        11,
        1884,
        257,
        8016,
        40999,
        11,
        293,
        550,
        2736,
        51028
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10524743004182799,
      "compression_ratio": 2.00836820083682,
      "no_speech_prob": 0.12923970818519592
    },
    {
      "id": 101,
      "seek": 58856,
      "start": 601.8399999999999,
      "end": 607.3599999999999,
      "text": " our own replication graph through that lambda. The other way that we can do it is that we can",
      "tokens": [
        51028,
        527,
        1065,
        39911,
        4295,
        807,
        300,
        13607,
        13,
        440,
        661,
        636,
        300,
        321,
        393,
        360,
        309,
        307,
        300,
        321,
        393,
        51304
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10524743004182799,
      "compression_ratio": 2.00836820083682,
      "no_speech_prob": 0.12923970818519592
    },
    {
      "id": 102,
      "seek": 58856,
      "start": 607.3599999999999,
      "end": 612.88,
      "text": " specify this through the default engine config any, but the kind of important discrepancy between",
      "tokens": [
        51304,
        16500,
        341,
        807,
        264,
        7576,
        2848,
        6662,
        604,
        11,
        457,
        264,
        733,
        295,
        1021,
        2983,
        265,
        6040,
        1344,
        1296,
        51580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10524743004182799,
      "compression_ratio": 2.00836820083682,
      "no_speech_prob": 0.12923970818519592
    },
    {
      "id": 103,
      "seek": 58856,
      "start": 612.88,
      "end": 618.3199999999999,
      "text": " these two things is that when you're doing it through your default engine any, this is game wide.",
      "tokens": [
        51580,
        613,
        732,
        721,
        307,
        300,
        562,
        291,
        434,
        884,
        309,
        807,
        428,
        7576,
        2848,
        604,
        11,
        341,
        307,
        1216,
        4874,
        13,
        51852
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10524743004182799,
      "compression_ratio": 2.00836820083682,
      "no_speech_prob": 0.12923970818519592
    },
    {
      "id": 104,
      "seek": 61832,
      "start": 618.32,
      "end": 623.84,
      "text": " So if you have a replication, you may want to use replication graphs different depending on",
      "tokens": [
        50364,
        407,
        498,
        291,
        362,
        257,
        39911,
        11,
        291,
        815,
        528,
        281,
        764,
        39911,
        24877,
        819,
        5413,
        322,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13657942379222196,
      "compression_ratio": 1.7699530516431925,
      "no_speech_prob": 0.00041043481905944645
    },
    {
      "id": 105,
      "seek": 61832,
      "start": 623.84,
      "end": 628.5600000000001,
      "text": " your gameplay scenario, maybe from level to level, then the default engine any doing it through your",
      "tokens": [
        50640,
        428,
        11421,
        9005,
        11,
        1310,
        490,
        1496,
        281,
        1496,
        11,
        550,
        264,
        7576,
        2848,
        604,
        884,
        309,
        807,
        428,
        50876
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13657942379222196,
      "compression_ratio": 1.7699530516431925,
      "no_speech_prob": 0.00041043481905944645
    },
    {
      "id": 106,
      "seek": 61832,
      "start": 628.5600000000001,
      "end": 632.96,
      "text": " any class is not really going to be suitable. That's the C++ is where you're going to be",
      "tokens": [
        50876,
        604,
        1508,
        307,
        406,
        534,
        516,
        281,
        312,
        12873,
        13,
        663,
        311,
        264,
        383,
        25472,
        307,
        689,
        291,
        434,
        516,
        281,
        312,
        51096
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13657942379222196,
      "compression_ratio": 1.7699530516431925,
      "no_speech_prob": 0.00041043481905944645
    },
    {
      "id": 107,
      "seek": 61832,
      "start": 632.96,
      "end": 637.12,
      "text": " wanting to do it if you maybe need to use a different replication graph in different scenarios.",
      "tokens": [
        51096,
        7935,
        281,
        360,
        309,
        498,
        291,
        1310,
        643,
        281,
        764,
        257,
        819,
        39911,
        4295,
        294,
        819,
        15077,
        13,
        51304
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13657942379222196,
      "compression_ratio": 1.7699530516431925,
      "no_speech_prob": 0.00041043481905944645
    }
  ],
  "language": "en"
}