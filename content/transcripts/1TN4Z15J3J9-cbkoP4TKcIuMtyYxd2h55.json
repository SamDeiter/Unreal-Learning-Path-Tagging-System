{
  "text": " Now, when working with alpha information inside of Unreal Engine 5, there are two ways to go about this. The first way is to use an embedded alpha, and an example of this is on the left-hand side of the screen. The embedded alpha works by storing the alpha information in the alpha channel of any texture, and then we sample that information by pulling from the alpha output of our texture sampler and wiring that into our opacity mask. And while this is a valid way to work, the issue is that any information that's stored inside of the alpha channel of a texture comes into Unreal uncompressed. This makes that texture take up roughly two times the amount of memory. Now, on the right-hand side of the screen, we have the same exact texture, but this time we have separated the alpha channel out into its own texture. And working like another material sampler inside of our material, we simply feed that information into our opacity mask. This has the added advantage of allowing us to have a cheaper texture footprint, and also allows us to do something interesting, which we're going to take a look at in this slide. Now, here we have an embedded versus a separate alpha for a leaf texture. And in the slide in front of us, everything looks kind of similar, but let's go ahead and take a look at this inside of Unreal to see what the advantage and disadvantage of these methods are. So inside of Unreal, here is our scene, and if we look at these in close inspection, they look pretty good. They're looking very similar to one another. So let's say that this one over here, which is our embedded, to open up its material so we can see it's just got a simple material set up. But say a technical director or technical artist, somebody comes by and says, hey, we need to get back some memory from our alpha. So what we can actually do with this is if we open up this texture here, come to our LOD bias, we can quickly just bias this down, and that is going to reduce this in size. Now, if we take a look at this and let's just compare by setting this back to zero. So we can see here that while it does take it down in size, it does reduce the quality of this a little bit. So inside of the branch here, we're losing a lot of the detail. It looks a little just kind of muddy versus setting this at zero here. So let's go ahead and leave that at one. Let's just close this. So over here, what we have is, again, the exact same setup, but slightly different in that it uses the alpha channel as a separate texture. So what we can do actually here is check this out. We can open up this alpha and if we come here and change our LOD bias to one, we've set this down to 85 KB in size. And if we take a look at that, we've got some little dots there, but we can try and fix that if we come here to our MIPGEN settings and set like a sharpen filter. Look at that. So we sharpened that back up. So what we have done is we have actually improved this by reducing it. We've made it take up less texture memory while still maintaining the exact same visual results. And this is really important because all of our optimizations should allow us to still have the artwork look the way the artist intended it. So we always want to try to make sure that when we're optimizing stuff, we're not destroying the artwork. And this is a great example of how we can allow artists freedom, but also have the freedom to make some performance optimizations should we need to based on content demands.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 8.0,
      "text": " Now, when working with alpha information inside of Unreal Engine 5, there are two ways to go about this.",
      "tokens": [
        50364,
        823,
        11,
        562,
        1364,
        365,
        8961,
        1589,
        1854,
        295,
        34464,
        7659,
        1025,
        11,
        456,
        366,
        732,
        2098,
        281,
        352,
        466,
        341,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09966591835021972,
      "compression_ratio": 1.8166666666666667,
      "no_speech_prob": 0.04882478341460228
    },
    {
      "id": 1,
      "seek": 0,
      "start": 8.0,
      "end": 13.0,
      "text": " The first way is to use an embedded alpha, and an example of this is on the left-hand side of the screen.",
      "tokens": [
        50764,
        440,
        700,
        636,
        307,
        281,
        764,
        364,
        16741,
        8961,
        11,
        293,
        364,
        1365,
        295,
        341,
        307,
        322,
        264,
        1411,
        12,
        5543,
        1252,
        295,
        264,
        2568,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09966591835021972,
      "compression_ratio": 1.8166666666666667,
      "no_speech_prob": 0.04882478341460228
    },
    {
      "id": 2,
      "seek": 0,
      "start": 13.0,
      "end": 19.0,
      "text": " The embedded alpha works by storing the alpha information in the alpha channel of any texture,",
      "tokens": [
        51014,
        440,
        16741,
        8961,
        1985,
        538,
        26085,
        264,
        8961,
        1589,
        294,
        264,
        8961,
        2269,
        295,
        604,
        8091,
        11,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09966591835021972,
      "compression_ratio": 1.8166666666666667,
      "no_speech_prob": 0.04882478341460228
    },
    {
      "id": 3,
      "seek": 0,
      "start": 19.0,
      "end": 27.0,
      "text": " and then we sample that information by pulling from the alpha output of our texture sampler and wiring that into our opacity mask.",
      "tokens": [
        51314,
        293,
        550,
        321,
        6889,
        300,
        1589,
        538,
        8407,
        490,
        264,
        8961,
        5598,
        295,
        527,
        8091,
        3247,
        22732,
        293,
        27520,
        300,
        666,
        527,
        41693,
        6094,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09966591835021972,
      "compression_ratio": 1.8166666666666667,
      "no_speech_prob": 0.04882478341460228
    },
    {
      "id": 4,
      "seek": 2700,
      "start": 27.0,
      "end": 37.0,
      "text": " And while this is a valid way to work, the issue is that any information that's stored inside of the alpha channel of a texture comes into Unreal uncompressed.",
      "tokens": [
        50364,
        400,
        1339,
        341,
        307,
        257,
        7363,
        636,
        281,
        589,
        11,
        264,
        2734,
        307,
        300,
        604,
        1589,
        300,
        311,
        12187,
        1854,
        295,
        264,
        8961,
        2269,
        295,
        257,
        8091,
        1487,
        666,
        34464,
        8585,
        79,
        3805,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07067242728339301,
      "compression_ratio": 1.6902654867256637,
      "no_speech_prob": 0.4068642854690552
    },
    {
      "id": 5,
      "seek": 2700,
      "start": 37.0,
      "end": 41.0,
      "text": " This makes that texture take up roughly two times the amount of memory.",
      "tokens": [
        50864,
        639,
        1669,
        300,
        8091,
        747,
        493,
        9810,
        732,
        1413,
        264,
        2372,
        295,
        4675,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07067242728339301,
      "compression_ratio": 1.6902654867256637,
      "no_speech_prob": 0.4068642854690552
    },
    {
      "id": 6,
      "seek": 2700,
      "start": 41.0,
      "end": 49.0,
      "text": " Now, on the right-hand side of the screen, we have the same exact texture, but this time we have separated the alpha channel out into its own texture.",
      "tokens": [
        51064,
        823,
        11,
        322,
        264,
        558,
        12,
        5543,
        1252,
        295,
        264,
        2568,
        11,
        321,
        362,
        264,
        912,
        1900,
        8091,
        11,
        457,
        341,
        565,
        321,
        362,
        12005,
        264,
        8961,
        2269,
        484,
        666,
        1080,
        1065,
        8091,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07067242728339301,
      "compression_ratio": 1.6902654867256637,
      "no_speech_prob": 0.4068642854690552
    },
    {
      "id": 7,
      "seek": 4900,
      "start": 50.0,
      "end": 58.0,
      "text": " And working like another material sampler inside of our material, we simply feed that information into our opacity mask.",
      "tokens": [
        50414,
        400,
        1364,
        411,
        1071,
        2527,
        3247,
        22732,
        1854,
        295,
        527,
        2527,
        11,
        321,
        2935,
        3154,
        300,
        1589,
        666,
        527,
        41693,
        6094,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07571039429630141,
      "compression_ratio": 1.6051502145922747,
      "no_speech_prob": 0.5768901705741882
    },
    {
      "id": 8,
      "seek": 4900,
      "start": 58.0,
      "end": 72.0,
      "text": " This has the added advantage of allowing us to have a cheaper texture footprint, and also allows us to do something interesting, which we're going to take a look at in this slide.",
      "tokens": [
        50814,
        639,
        575,
        264,
        3869,
        5002,
        295,
        8293,
        505,
        281,
        362,
        257,
        12284,
        8091,
        24222,
        11,
        293,
        611,
        4045,
        505,
        281,
        360,
        746,
        1880,
        11,
        597,
        321,
        434,
        516,
        281,
        747,
        257,
        574,
        412,
        294,
        341,
        4137,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07571039429630141,
      "compression_ratio": 1.6051502145922747,
      "no_speech_prob": 0.5768901705741882
    },
    {
      "id": 9,
      "seek": 4900,
      "start": 72.0,
      "end": 77.0,
      "text": " Now, here we have an embedded versus a separate alpha for a leaf texture.",
      "tokens": [
        51514,
        823,
        11,
        510,
        321,
        362,
        364,
        16741,
        5717,
        257,
        4994,
        8961,
        337,
        257,
        10871,
        8091,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07571039429630141,
      "compression_ratio": 1.6051502145922747,
      "no_speech_prob": 0.5768901705741882
    },
    {
      "id": 10,
      "seek": 7700,
      "start": 77.0,
      "end": 87.0,
      "text": " And in the slide in front of us, everything looks kind of similar, but let's go ahead and take a look at this inside of Unreal to see what the advantage and disadvantage of these methods are.",
      "tokens": [
        50364,
        400,
        294,
        264,
        4137,
        294,
        1868,
        295,
        505,
        11,
        1203,
        1542,
        733,
        295,
        2531,
        11,
        457,
        718,
        311,
        352,
        2286,
        293,
        747,
        257,
        574,
        412,
        341,
        1854,
        295,
        34464,
        281,
        536,
        437,
        264,
        5002,
        293,
        24292,
        295,
        613,
        7150,
        366,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05932333890129538,
      "compression_ratio": 1.7822878228782288,
      "no_speech_prob": 0.1686742901802063
    },
    {
      "id": 11,
      "seek": 7700,
      "start": 87.0,
      "end": 93.0,
      "text": " So inside of Unreal, here is our scene, and if we look at these in close inspection, they look pretty good.",
      "tokens": [
        50864,
        407,
        1854,
        295,
        34464,
        11,
        510,
        307,
        527,
        4145,
        11,
        293,
        498,
        321,
        574,
        412,
        613,
        294,
        1998,
        22085,
        11,
        436,
        574,
        1238,
        665,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05932333890129538,
      "compression_ratio": 1.7822878228782288,
      "no_speech_prob": 0.1686742901802063
    },
    {
      "id": 12,
      "seek": 7700,
      "start": 93.0,
      "end": 95.0,
      "text": " They're looking very similar to one another.",
      "tokens": [
        51164,
        814,
        434,
        1237,
        588,
        2531,
        281,
        472,
        1071,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05932333890129538,
      "compression_ratio": 1.7822878228782288,
      "no_speech_prob": 0.1686742901802063
    },
    {
      "id": 13,
      "seek": 7700,
      "start": 95.0,
      "end": 102.0,
      "text": " So let's say that this one over here, which is our embedded, to open up its material so we can see it's just got a simple material set up.",
      "tokens": [
        51264,
        407,
        718,
        311,
        584,
        300,
        341,
        472,
        670,
        510,
        11,
        597,
        307,
        527,
        16741,
        11,
        281,
        1269,
        493,
        1080,
        2527,
        370,
        321,
        393,
        536,
        309,
        311,
        445,
        658,
        257,
        2199,
        2527,
        992,
        493,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05932333890129538,
      "compression_ratio": 1.7822878228782288,
      "no_speech_prob": 0.1686742901802063
    },
    {
      "id": 14,
      "seek": 10200,
      "start": 102.0,
      "end": 109.0,
      "text": " But say a technical director or technical artist, somebody comes by and says, hey, we need to get back some memory from our alpha.",
      "tokens": [
        50364,
        583,
        584,
        257,
        6191,
        5391,
        420,
        6191,
        5748,
        11,
        2618,
        1487,
        538,
        293,
        1619,
        11,
        4177,
        11,
        321,
        643,
        281,
        483,
        646,
        512,
        4675,
        490,
        527,
        8961,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06552711246520515,
      "compression_ratio": 1.7403508771929825,
      "no_speech_prob": 0.160131573677063
    },
    {
      "id": 15,
      "seek": 10200,
      "start": 109.0,
      "end": 119.0,
      "text": " So what we can actually do with this is if we open up this texture here, come to our LOD bias, we can quickly just bias this down, and that is going to reduce this in size.",
      "tokens": [
        50714,
        407,
        437,
        321,
        393,
        767,
        360,
        365,
        341,
        307,
        498,
        321,
        1269,
        493,
        341,
        8091,
        510,
        11,
        808,
        281,
        527,
        441,
        14632,
        12577,
        11,
        321,
        393,
        2661,
        445,
        12577,
        341,
        760,
        11,
        293,
        300,
        307,
        516,
        281,
        5407,
        341,
        294,
        2744,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06552711246520515,
      "compression_ratio": 1.7403508771929825,
      "no_speech_prob": 0.160131573677063
    },
    {
      "id": 16,
      "seek": 10200,
      "start": 119.0,
      "end": 125.0,
      "text": " Now, if we take a look at this and let's just compare by setting this back to zero.",
      "tokens": [
        51214,
        823,
        11,
        498,
        321,
        747,
        257,
        574,
        412,
        341,
        293,
        718,
        311,
        445,
        6794,
        538,
        3287,
        341,
        646,
        281,
        4018,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06552711246520515,
      "compression_ratio": 1.7403508771929825,
      "no_speech_prob": 0.160131573677063
    },
    {
      "id": 17,
      "seek": 10200,
      "start": 125.0,
      "end": 131.0,
      "text": " So we can see here that while it does take it down in size, it does reduce the quality of this a little bit.",
      "tokens": [
        51514,
        407,
        321,
        393,
        536,
        510,
        300,
        1339,
        309,
        775,
        747,
        309,
        760,
        294,
        2744,
        11,
        309,
        775,
        5407,
        264,
        3125,
        295,
        341,
        257,
        707,
        857,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06552711246520515,
      "compression_ratio": 1.7403508771929825,
      "no_speech_prob": 0.160131573677063
    },
    {
      "id": 18,
      "seek": 13100,
      "start": 131.0,
      "end": 136.0,
      "text": " So inside of the branch here, we're losing a lot of the detail.",
      "tokens": [
        50364,
        407,
        1854,
        295,
        264,
        9819,
        510,
        11,
        321,
        434,
        7027,
        257,
        688,
        295,
        264,
        2607,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.062361383901059046,
      "compression_ratio": 1.6276150627615062,
      "no_speech_prob": 0.005219381768256426
    },
    {
      "id": 19,
      "seek": 13100,
      "start": 136.0,
      "end": 141.0,
      "text": " It looks a little just kind of muddy versus setting this at zero here.",
      "tokens": [
        50614,
        467,
        1542,
        257,
        707,
        445,
        733,
        295,
        38540,
        5717,
        3287,
        341,
        412,
        4018,
        510,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.062361383901059046,
      "compression_ratio": 1.6276150627615062,
      "no_speech_prob": 0.005219381768256426
    },
    {
      "id": 20,
      "seek": 13100,
      "start": 141.0,
      "end": 144.0,
      "text": " So let's go ahead and leave that at one. Let's just close this.",
      "tokens": [
        50864,
        407,
        718,
        311,
        352,
        2286,
        293,
        1856,
        300,
        412,
        472,
        13,
        961,
        311,
        445,
        1998,
        341,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.062361383901059046,
      "compression_ratio": 1.6276150627615062,
      "no_speech_prob": 0.005219381768256426
    },
    {
      "id": 21,
      "seek": 13100,
      "start": 144.0,
      "end": 154.0,
      "text": " So over here, what we have is, again, the exact same setup, but slightly different in that it uses the alpha channel as a separate texture.",
      "tokens": [
        51014,
        407,
        670,
        510,
        11,
        437,
        321,
        362,
        307,
        11,
        797,
        11,
        264,
        1900,
        912,
        8657,
        11,
        457,
        4748,
        819,
        294,
        300,
        309,
        4960,
        264,
        8961,
        2269,
        382,
        257,
        4994,
        8091,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.062361383901059046,
      "compression_ratio": 1.6276150627615062,
      "no_speech_prob": 0.005219381768256426
    },
    {
      "id": 22,
      "seek": 13100,
      "start": 154.0,
      "end": 156.0,
      "text": " So what we can do actually here is check this out.",
      "tokens": [
        51514,
        407,
        437,
        321,
        393,
        360,
        767,
        510,
        307,
        1520,
        341,
        484,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.062361383901059046,
      "compression_ratio": 1.6276150627615062,
      "no_speech_prob": 0.005219381768256426
    },
    {
      "id": 23,
      "seek": 15600,
      "start": 156.0,
      "end": 165.0,
      "text": " We can open up this alpha and if we come here and change our LOD bias to one, we've set this down to 85 KB in size.",
      "tokens": [
        50364,
        492,
        393,
        1269,
        493,
        341,
        8961,
        293,
        498,
        321,
        808,
        510,
        293,
        1319,
        527,
        441,
        14632,
        12577,
        281,
        472,
        11,
        321,
        600,
        992,
        341,
        760,
        281,
        14695,
        591,
        33,
        294,
        2744,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11204851434585895,
      "compression_ratio": 1.5631067961165048,
      "no_speech_prob": 0.28759750723838806
    },
    {
      "id": 24,
      "seek": 15600,
      "start": 165.0,
      "end": 176.0,
      "text": " And if we take a look at that, we've got some little dots there, but we can try and fix that if we come here to our MIPGEN settings and set like a sharpen filter.",
      "tokens": [
        50814,
        400,
        498,
        321,
        747,
        257,
        574,
        412,
        300,
        11,
        321,
        600,
        658,
        512,
        707,
        15026,
        456,
        11,
        457,
        321,
        393,
        853,
        293,
        3191,
        300,
        498,
        321,
        808,
        510,
        281,
        527,
        376,
        9139,
        38,
        2195,
        6257,
        293,
        992,
        411,
        257,
        31570,
        6608,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11204851434585895,
      "compression_ratio": 1.5631067961165048,
      "no_speech_prob": 0.28759750723838806
    },
    {
      "id": 25,
      "seek": 15600,
      "start": 176.0,
      "end": 178.0,
      "text": " Look at that. So we sharpened that back up.",
      "tokens": [
        51364,
        2053,
        412,
        300,
        13,
        407,
        321,
        31570,
        292,
        300,
        646,
        493,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.11204851434585895,
      "compression_ratio": 1.5631067961165048,
      "no_speech_prob": 0.28759750723838806
    },
    {
      "id": 26,
      "seek": 17800,
      "start": 178.0,
      "end": 184.0,
      "text": " So what we have done is we have actually improved this by reducing it.",
      "tokens": [
        50364,
        407,
        437,
        321,
        362,
        1096,
        307,
        321,
        362,
        767,
        9689,
        341,
        538,
        12245,
        309,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.029926446176344348,
      "compression_ratio": 1.6693548387096775,
      "no_speech_prob": 0.32050225138664246
    },
    {
      "id": 27,
      "seek": 17800,
      "start": 184.0,
      "end": 191.0,
      "text": " We've made it take up less texture memory while still maintaining the exact same visual results.",
      "tokens": [
        50664,
        492,
        600,
        1027,
        309,
        747,
        493,
        1570,
        8091,
        4675,
        1339,
        920,
        14916,
        264,
        1900,
        912,
        5056,
        3542,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.029926446176344348,
      "compression_ratio": 1.6693548387096775,
      "no_speech_prob": 0.32050225138664246
    },
    {
      "id": 28,
      "seek": 17800,
      "start": 191.0,
      "end": 199.0,
      "text": " And this is really important because all of our optimizations should allow us to still have the artwork look the way the artist intended it.",
      "tokens": [
        51014,
        400,
        341,
        307,
        534,
        1021,
        570,
        439,
        295,
        527,
        5028,
        14455,
        820,
        2089,
        505,
        281,
        920,
        362,
        264,
        15829,
        574,
        264,
        636,
        264,
        5748,
        10226,
        309,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.029926446176344348,
      "compression_ratio": 1.6693548387096775,
      "no_speech_prob": 0.32050225138664246
    },
    {
      "id": 29,
      "seek": 17800,
      "start": 199.0,
      "end": 204.0,
      "text": " So we always want to try to make sure that when we're optimizing stuff, we're not destroying the artwork.",
      "tokens": [
        51414,
        407,
        321,
        1009,
        528,
        281,
        853,
        281,
        652,
        988,
        300,
        562,
        321,
        434,
        40425,
        1507,
        11,
        321,
        434,
        406,
        19926,
        264,
        15829,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.029926446176344348,
      "compression_ratio": 1.6693548387096775,
      "no_speech_prob": 0.32050225138664246
    },
    {
      "id": 30,
      "seek": 20400,
      "start": 204.0,
      "end": 216.0,
      "text": " And this is a great example of how we can allow artists freedom, but also have the freedom to make some performance optimizations should we need to based on content demands.",
      "tokens": [
        50364,
        400,
        341,
        307,
        257,
        869,
        1365,
        295,
        577,
        321,
        393,
        2089,
        6910,
        5645,
        11,
        457,
        611,
        362,
        264,
        5645,
        281,
        652,
        512,
        3389,
        5028,
        14455,
        820,
        321,
        643,
        281,
        2361,
        322,
        2701,
        15107,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07041344771514067,
      "compression_ratio": 1.3515625,
      "no_speech_prob": 0.38329532742500305
    }
  ],
  "language": "en"
}